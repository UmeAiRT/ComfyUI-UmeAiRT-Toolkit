
import importlib.util
import torch
import sys
import os
import contextlib
from .logger import log_node, GREEN, RED, YELLOW, RESET, CYAN, MAGENTA

# Global Cache
_LIB_CACHE = {}

def check_library(name):
    """Checks if a specific Python library module is installed and caches the result.

    Args:
        name (str): The module name to check (e.g., "sageattention", "triton").

    Returns:
        bool: True if the module is found in the current environment, False otherwise.
    """
    if name in _LIB_CACHE:
        return _LIB_CACHE[name]
    
    spec = importlib.util.find_spec(name)
    found = spec is not None
    _LIB_CACHE[name] = found
    return found

def get_cuda_memory():
    """Retrieves the current CUDA memory usage.

    Returns:
        str: A formatted string showing free and total VRAM in Gigabytes (e.g., '10.50GB free / 24.00GB total').
             Returns "CUDA not available" if torch.cuda is not initialized, or "Unknown" upon failure.
    """
    try:
        if torch.cuda.is_available():
            free, total = torch.cuda.mem_get_info()
            free_gb = free / (1024 ** 3)
            total_gb = total / (1024 ** 3)
            return f"{free_gb:.2f}GB free / {total_gb:.2f}GB total"
        return "CUDA not available"
    except Exception:
        return "Unknown"

def check_optimizations():
    """Checks for optimization libraries and system status during startup.

    Logs the presence of crucial performance libraries like SageAttention, Flash Attention, 
    and Triton. It also prints out the initial CUDA memory state to inform the user.
    """
    
    # 1. Library Checks
    sage_status = f"{GREEN}‚úÖ{RESET}" if check_library("sageattention") else f"{RED}‚ùå{RESET}"
    flash_status = f"{GREEN}‚úÖ{RESET}" if check_library("flash_attn") else f"{RED}‚ùå{RESET}"
    triton_status = f"{GREEN}‚úÖ{RESET}" if check_library("triton") else f"{RED}‚ùå{RESET}"
    
    # Using print directly for specific formatting needs or log_node if we want the prefix
    # The user request showed specific icon prefixes. usage of log_node adds [UmeAiRT-Toolkit] prefix.
    # We will use print for the custom lines to match the requested look exactly, 
    # but maybe keep consistent with the project style which uses print in __init__.py too.
    
    log_node(f"‚ö° Optimisation check: SageAttention {sage_status} | Flash Attention {flash_status} | Triton {triton_status}")
    
    if not check_library("flash_attn"):
        log_node(f"üí° Optional: pip install flash-attn", color="YELLOW")

    # 3. CUDA Memory
    mem_str = get_cuda_memory()
    log_node(f"üìä Initial CUDA memory: {mem_str}")

    # Return summary for HealthCheck usage
    sage = "‚úÖ" if check_library("sageattention") else "‚ùå"
    flash = "‚úÖ" if check_library("flash_attn") else "‚ùå"
    triton = "‚úÖ" if check_library("triton") else "‚ùå"
    return f"SageAttn={sage} | Flash={flash} | Triton={triton}"


# --- Target-Resolution VAE Warmup ---
_WARMED_UP_SHAPES = set()

def warmup_vae(vae, latent_image):
    """Forces an initial VAE compilation pass using an empty tensor of the accurate target resolution.

    This prevents JIT compilers (like Triton or SageAttention) from stalling heavily or spiking VRAM 
    during the first generated frame. By pre-compiling the exact (C, H, W) kernel sizes, 
    subsequent generations are fluid and out-of-memory spikes are avoided.

    Args:
        vae (comfy.sd.VAE): The loaded VAE instance from ComfyUI.
        latent_image (dict): A dictionary containing at least a "samples" key with the target latent shape.
    """
    if not isinstance(latent_image, dict) or "samples" not in latent_image:
        return
        
    if not check_library("triton"):
        return # No Triton = No heavy JIT stall

    try:
        # Extract the exact shape (B, C, H, W) of the target image
        shape = tuple(latent_image["samples"].shape)
        if len(shape) != 4: return
        B, C, H, W = shape
        
        # If we already compiled this exact resolution this session, skip
        if (H, W) in _WARMED_UP_SHAPES:
            return
            
        log_node(f"‚ö° Optimisation: VAE Target-Resolution Warmup {H*8}x{W*8} (Preventing VRAM spikes)...", color="CYAN")
        
        import comfy_extras.nodes_custom_sampler as comfy_nodes
        import nodes
        
        target_device = latent_image["samples"].device
        
        try:
            # Attempt 1: 16 Channels (FLUX)
            empty_16 = torch.zeros([B, 16, H, W], device=target_device)
            nodes.VAEDecode().decode(vae, {"samples": empty_16})
        except Exception as e:
            if "channels" in str(e).lower() or "size" in str(e).lower() or "dimension" in str(e).lower():
                # Attempt 2: 4 Channels (SD1.5 / SDXL)
                empty_4 = torch.zeros([B, 4, H, W], device=target_device)
                nodes.VAEDecode().decode(vae, {"samples": empty_4})
            else:
                raise e
        
        # Remember this shape so we don't compile it again
        _WARMED_UP_SHAPES.add((H, W))
        
    except Exception as e:
        log_node(f"VAE Warmup failed (Safe to ignore): {e}", color="YELLOW")

# --- Sampler Context (Optimizations) ---

class SamplerContext:
    """Context manager to enable compatible optimizations (e.g., SageAttention) during sampling."""
    def __init__(self):
        self.original_sdpa = getattr(torch.nn.functional, 'scaled_dot_product_attention', None)
        self.optimization_name = "None"
        
    def __enter__(self):
        if check_library("sageattention"):
            self.optimization_name = "SageAttention"
            log_node("‚ö° Optimisation Active: SageAttention", color="MAGENTA")
            try:
                # Many forks of SageAttention for ComfyUI use simple import hooks or patchers.
                # Just importing the patcher often works, or we directly apply it if known.
                # In standard usage with SD, users use `import sageattention` or similar.
                # We'll just patch F.scaled_dot_product_attention if sageattn is available as a function
                from sageattention import sageattn
                torch.nn.functional.scaled_dot_product_attention = sageattn
            except ImportError:
                # If there's no sageattn function, it might auto-patch on import.
                pass
            except Exception:
                pass
        elif check_library("triton"):
            self.optimization_name = "Triton"
            # Triton is often handled behind the scenes by comfy backend if available, so we just log it
            log_node("‚ö° Optimisation Active: Triton", color="MAGENTA")
        else:
            self.optimization_name = "Default"
            
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.optimization_name == "SageAttention" and self.original_sdpa is not None:
             # Restore native SDPA to prevent breaking other nodes down the line
             torch.nn.functional.scaled_dot_product_attention = self.original_sdpa

